---
title: "Final Project Social Media"
author: "hp621@scarletmail.rutgers.edu"
date: "26/04/2024"
output: html_document
---
#Imported all the necessary Library
```{r}
library(psych)
library(dplyr)
library(devtools)
library(ggplot2)
library(tidyr)
library(FactoMineR)
library(tidyr)
library(MASS)
library(cluster)
library(magrittr)
library(NbClust)
library(factoextra)
library(psych)
library(GGally)
library(car)
library(caret)
library(e1071)
library(pROC)
library(readxl)
library(Hotelling)
library(corrplot)
library(memisc)
library(ROCR)
library(klaR)
library(pROC)
```
#Importing the data
The data set was gathered by students from the class. The students had filled the Google form for a month and data was cleaned. The curiosity arise between students and Prof. Ronak Parikh regarding the usage of Social media Furthermore is it good or bad to use social media and What are the major apps running in the market which affects the daily routine of  a person.It has 22 observation of 13 variable 
```{r}
social_media <- read_excel("social_media_cleaned.xlsx")

```

Desribe data
```{r}
dim(social_media)
#It has 22 rows and 13 columns

colnames(social_media)
#The name of following columns of the dataset


str(social_media)

#It shows the columns character is in character while others are in numeric data


summary(social_media)

attach(social_media)
```

```{r}


social_media %>%
  dplyr::select(2:13) %>%
  pivot_longer(everything()) %>%
  ggplot(aes(x = value))+
  geom_histogram(bins = 30, color = "black", fill = "skyblue") +
  facet_wrap(~name, scales = "free")
```
#We can identify usage of Instagram, Linkedin, whatsapp, youtube are used highest used
#How you felt entire week is factor 


```{r}
corr_plot_data <- social_media %>%  dplyr::select(How.you.felt.the.entire.week, Instagram, LinkedIn, SnapChat,Twitter,Whatsapp, youtube,Trouble.falling.asleep, Reddit, Tired.waking.up.in.morning, OTT, Mood.Productivity)

corr_matrix <- cor(corr_plot_data)

corrplot(corr_matrix, 
         method = "color",  
         type = "upper", 
         order = "hclust",
         diag = TRUE,
         addCoef.col = TRUE,
         number.cex = 0.55,
         tl.srt = 60)
```
Mood productivity is negativity correlated with whatsapp and OTT
Trouble falling asleep is potivity correlated with snapchat 

```{r}
#computing distance between the dataset 

scale <- scale(social_media[-1])
#mean of each column
socialmean <- colMeans(social_media[-1])
#correlation of each column
socialcor <- cor(social_media[-1])
#covariance
socialcov <- cov(social_media[-1])



# Computing the Mahalanobis distance for each country
mahalanobis_distance <- function(x) {
  mahalanobis(x, socialmean, socialcov)
}

# Applying the Mahalanobis distance function to each row of the dataset
distance_matrix <- apply(social_media[-1], 1, mahalanobis_distance)

# Converting the distance matrix to a data frame for better readability
distance_df <- data.frame(character , Distance = distance_matrix)

# Displaying the Mahalanobis distance for each column
print(distance_df)

plot(distance_df)
```

#These are the distance of each columns across the dimensions




```{r}
rownames(scale) <- social_media$character

# we take 2 clusters, k-means = 2
kmeans2.social <- kmeans(scale,2,nstart = 10)

# Computing the percentage of variation accounted for two  clusters
perc.var.2 <- round(100*(1 - kmeans2.social$betweenss/kmeans2.social$totss),1)
names(perc.var.2) <- "Perc. 2 clus"
perc.var.2

# we take 3 clusters, k-means = 3
kmeans3.social <- kmeans(scale,3,nstart = 10)
# Computing the percentage of variation accounted for three  clusters
perc.var.3 <- round(100*(1 - kmeans3.social$betweenss/kmeans3.social$totss),1)
names(perc.var.3) <- "Perc. 3 clus"
perc.var.3

# For 4 clusters, k-means = 4
kmeans4.social <- kmeans(scale,4,nstart = 10)
# Computing the percentage of variation accounted for four clusters
perc.var.4 <- round(100*(1 - kmeans4.social$betweenss/kmeans4.social$totss),1)
names(perc.var.4) <- "Perc. 4 clus"
perc.var.4

#For 5 clusters, k-means = 5
kmeans5.social <- kmeans(scale,5,nstart = 10)
# Computing the percentage of variation accounted for four clusters
perc.var.5 <- round(100*(1 - kmeans5.social$betweenss/kmeans5.social$totss),1)
names(perc.var.5) <- "Perc. 5 clus"
perc.var.5

```
it seems like the biggest drop occurs between 2 and 3 clusters, and the rate of decrease decreases further after 3 clusters. Therefore, based on the Elbow Method, you might choose 3 clusters as the optimal number.

```{r}

# We divide the dataset into three clusters.
# Filtering properties which are in 1 cluster of k mean 3
clus1 <- matrix(names(kmeans3.social$cluster[kmeans3.social$cluster == 1]),
                ncol=1, nrow=length(kmeans3.social$cluster[kmeans3.social$cluster == 1]))
colnames(clus1) <- "Cluster 1"
clus1

# Filtering properties which are in 2 cluster of k mean 3
clus2 <- matrix(names(kmeans2.social$cluster[kmeans2.social$cluster == 2]),
                ncol=1, nrow=length(kmeans2.social$cluster[kmeans2.social$cluster == 2]))
colnames(clus2) <- "Cluster 2"
clus2

# Filtering properties which are in 3 cluster of k mean 3
clus3 <- matrix(names(kmeans3.social$cluster[kmeans3.social$cluster == 3]),
                ncol=1, nrow=length(kmeans3.social$cluster[kmeans2.social$cluster == 3]))
colnames(clus3) <- "Cluster 3"
clus3






```
```{r}

# We can use the hierarchical clustering method, which allows us to create a dendrogram 

# Hierarchical Clustering

# Calculating the distance matrix using Euclidean distance
dist.social <- dist(social_media, method = "euclidean") # Distance matrix

# Performing hierarchical clustering using complete linkage
fit <- hclust(dist.social, method="complete")

#Plotting the dendogram

plot(fit, main="Dendrogram of characters based on social data")

#Cutting the tree into 3 clusters
groups <- cutree(fit, k=3)

#Plotting dendogram with red borders around the 2 clusters
rect.hclust(fit, k=3, border="red")

gap_stat <- clusGap(scale, FUN = kmeans, nstart = 1, K.max = 6, B = 50)

fviz_gap_stat(gap_stat)

km.res3 <- kmeans(scale, 3, nstart = 10)  

# Visualize clusters
fviz_cluster(km.res3, data = scale,  
             ellipse.type = "convex",
             palette = "jco",
             ggtheme = theme_minimal())


```

```{r}
#â€¢	Use principal components analysis to investigate the relationships between the characters based on these variables. 

# Getting the Correlations 

social_plot <- socialcor

corrplot(social_plot,tl.cex = 0.7)
```



```{r}
social.PCA <- prcomp(social_media[-1],scale=TRUE)
social.PCA

str(social.PCA)
#sdev,rotation,center,scale, x

social.eigen <- social.PCA$sdev^2
social.eigen

sumlambdas <- sum(social.eigen)
sumlambdas
#10

propvar <- social.eigen/sumlambdas
propvar

cumvar <- cumsum(propvar)
cumvar

matlambdas <- rbind(social.eigen,propvar,cumvar)
rownames(matlambdas) <- c("Eigenvalues","Prop. variance","Cum. prop. variance")
round(matlambdas,4)
summary(social.PCA)
social.PCA$rotation
print(social.PCA)





plot(social.eigen, xlab = "Component number", ylab = "Component variance", type = "l", main = "Scree diagram")

#cutoff for PCA dimension's should be at 2 the elbow break is massive after 2 dimension's indicating that the variance dropp off is massive

res.pca <- PCA(social_media[-1], graph = FALSE)

fviz_pca_var(res.pca, col.var = "black")

plot(log(social.eigen), xlab = "Component number",ylab = "log(Component variance)", type="l",main = "Log(eigenvalue) diagram")

# Correlation
pairs.panels(social_media[,-1],
             gap = 0,
             bg = c("red", "blue")[social_media$character],
             pch=21)

pairs.panels(social.PCA$x,
             gap=0,
             bg = c("red", "blue")[social_media$character],
             pch=21)

# Variables - PCA (cos2)
fviz_eig(social.PCA, addlabels = TRUE)
fviz_pca_var(social.PCA,col.var = "cos2",
             gradient.cols = c("#FFCC00", "#CC9933", "#660033", "#330033"),
             repel = TRUE)
# Individuals - PCA (cos2)
fviz_pca_ind(social.PCA, col.ind = "cos2", 
             gradient.cols = c("#FFCC00", "#CC9933", "#660033", "#330033"), 
             repel = TRUE)



```
# Principal Component Analysis is used to reduce the large set of variables to a 
# small set that still contains most of the information from the large dataset and thereby, reducing the dimension of the dataset.
# After performing PCA on the  Dataset, we obtain 10 Principal components. 
# Each of these components represent the percentage of variability present in the dataset.
# In other words, PC1 explains 24.5% of total variance, PC2 explains 17.8% and PC3 explains 14.8%. 
# We will consider the first three principal components as they sum up to 57.1 of total variance 
# and the others can be discarded as they contribute to only minimal amount of total variance.


```{r}
#	Identify the important factors underlying the observed variables and examine the relationships between the characters to these factors. Show visualizations to support your answers where possible.


fit.pc <- principal(social_media[-1], nfactors=6, rotate="varimax")
fit.pc

fit.pc1 <- principal(social_media[-1], nfactors=5, rotate="varimax")
fit.pc1

fit.pc2 <- principal(social_media[-1], nfactors=4, rotate="varimax")
fit.pc2

# Factor recommendations for a simple structure
vss(social_media[-1]) 

# We are considering 4 factors, because of VSS (Very Simple Structure)
round(fit.pc$values, 4)



# Loadings
fit.pc$loadings
# Factor Loadings: This table displays the factor loadings for each variable. 
# It represents the correlation between each variable and each factor. 

# Communalities
fit.pc$communality
# Communalities are estimates of the variance in each observed variable that can be 
# explained by the extracted factors.
# It shows the proportion of each variable's variance that can be explained by the 


# Rotated factor scores
head(fit.pc$scores)
round(fit.pc$values,4)

# Factor recommendation
fa.parallel(social_media[-1])
# From this, we can inference that 4th component could be the best choice for number of factors

# Correlations within Factors
fa.plot(fit.pc)

# Visualizing the relationship
fa.diagram(fit.pc)


summary(vss(social_media[-1]))
```


#Multiple regression and analysis
```{r}
fit <- lm(How.you.felt.the.entire.week ~ Instagram+Whatsapp+SnapChat+LinkedIn+youtube,social_media,scale= TRUE)

summary(fit)
```
none of the predictor variables are statistically significant based on their p-values (all p-values are greater than 0.05). This suggests that the model doesn't provide a good explanation of the variation in the dependent variable based on the predictor variables included


```{r}
coefficients(fit)

```

```{r}
ggpairs(data=social_media[-1], title="Social Media Data")
confint(fit,level=0.95)
fitted(fit)
residuals(fit)
```


```{r}

residuals(fit)
#Anova Table
anova(fit)
vcov(fit)
cov2cor(vcov(fit))
temp <- influence.measures(fit)
temp
plot(fit)
```
```{r}

# Global test of model assumptions
library(gvlma)

gvmodel <- gvlma(fit)
summary(gvmodel)
fit
summary(fit)
fit1 <- fit
fit2 <- lm(How.you.felt.the.entire.week ~ Instagram+Whatsapp+LinkedIn+youtube,social_media,scale= TRUE)
```
```{r}
# compare models
anova(fit1, fit2)

step <- stepAIC(fit, direction="both")
step$anova # display results
```







```{r}
library(leaps)
leaps<-regsubsets(How.you.felt.the.entire.week ~ Instagram+Whatsapp+SnapChat+LinkedIn+youtube,data=social_media,nbest=10)
leaps
# view results
plot(leaps)
plot(leaps,scale="r2")
plot(leaps,scale="bic")
summary(leaps)
```
```{r}
library(relaimpo)
calc.relimp(fit,type=c("lmg","last","first","pratt"),
            rela=TRUE)
```
```{r}
summary(fit)
predict.lm(fit, data.frame( Instagram=13.7,Whatsapp=3.5,SnapChat=2.20,LinkedIn=12.5,youtube=6.7) )
```
```{r}
rsquared <- summary(fit)$r.squared
cat("R-squared:", rsquared, "\n")


adjusted_rsquared <- summary(fit)$adj.r.squared
cat("Adjusted R-squared:", adjusted_rsquared, "\n")
```

#logistic regression
```{r}
#Explolatory Analysis

social_media$Mood.Productivity <- as.factor(social_media$Mood.Productivity)
social_media$Tired.waking.up.in.morning<- as.factor(social_media$Tired.waking.up.in.morning)
```


```{r}
xtabs(~Mood.Productivity + Instagram, data=social_media)
xtabs(~Mood.Productivity + SnapChat, data=social_media)
xtabs(~Mood.Productivity + LinkedIn, data=social_media)
xtabs(~Mood.Productivity + OTT, data=social_media)
xtabs(~Mood.Productivity + Whatsapp, data=social_media)
xtabs(~Mood.Productivity + youtube, data=social_media)
xtabs(~Mood.Productivity + Twitter, data=social_media)
xtabs(~Mood.Productivity + Reddit, data=social_media)



```

```{r}
logistic_simple <- glm(Mood.Productivity ~ Instagram , data=social_media, family = 'binomial')
summary(logistic_simple)
```


```{r}
predicted.data <- data.frame(probability.of.Mood = logistic_simple$fitted.values,
                              Instagram = social_media$Instagram)
print(predicted.data)
```

```{r}

# We can plot the data
ggplot(data=predicted.data ,aes(x=Instagram, y=probability.of.Mood))+ geom_point(aes(color=Instagram), size=5) + xlab("Instagram") + ylab("Predicted probability of Mood")

xtabs(~ probability.of.Mood + Instagram , data=predicted.data)


```

```{r}
logistic <- glm(Mood.Productivity ~ ., data=social_media, family="binomial")
summary(logistic)
```
```{r}
predicted.data <- data.frame(probability.of.ls=logistic$fitted.values,ls=social_media$Mood.Productivity)
predicted.data <- predicted.data[order(predicted.data$probability.of.ls, decreasing=FALSE),]
predicted.data$rank <- 1:nrow(predicted.data)
## Lastly, we can plot the predicted probabilities for each sample having
## heart disease and color by whether or not they actually had heart disease
ggplot(data=predicted.data, aes(x=rank, y=probability.of.ls)) +
geom_point(aes(color=ls), alpha=1, shape=4, stroke=2) +
xlab("Index") +
ylab("Predicted probability of what mmood you get in usage")
```

```{r}
conf_mat <- table(Actual = social_media$Mood.Productivity, Predicted = ifelse(logistic$fitted.values > 0.5, "Single", "Album"))
print(conf_mat)


# Calculate accuracy
accuracy <- sum(diag(conf_mat)) / sum(conf_mat)
print(paste("Accuracy:", accuracy))


roc_obj <- roc(social_media$Mood.Productivity, logistic$fitted.values)
plot(roc_obj, xlab="False Positive Percentage", ylab="True Postive Percentage", legacy.axes=TRUE, col="#377eb8", lwd=4, percent=TRUE, print.auc=TRUE)







```
#Disriminant Analysis

```{r}
set.seed(42)

smp_size_raw <- floor(0.75 * nrow(social_media))

train_ind_raw <- sample(nrow(social_media), size = smp_size_raw)

#Creating Train and Test Data
train_raw.df <- as.data.frame(social_media[train_ind_raw, ])
test_raw.df <- as.data.frame(social_media[-train_ind_raw, ])
```



```{r}
sm_lda <- lda(formula = train_raw.df$Mood.Productivity ~ ., data = train_raw.df)
sm_lda

summary(sm_lda)
plot(sm_lda)
plot(sm_lda, col = as.integer(train_raw.df$Instagram))
partimat(Mood.Productivity ~ Instagram+SnapChat+Whatsapp+youtube+LinkedIn+OTT , data = train_raw.df, method = "lda")
```
```{r}
sm.lda.predict <- predict(sm_lda, data = test_raw.df$Mood.Productivity)
sm.lda.predict$class

sm.lda.predict$x


# Predict class labels for the test dataset
predicted_labels <- predict(sm_lda, data = test_raw.df)$class

# Calculate accuracy
actual_labels <- test_raw.df$Mood.Productivity
accuracy <- mean(predicted_labels == actual_labels)

cat("Accuracy:", accuracy, "\n")
```

